---
title: "Gaussians all mixed up: a first look"
date: "2025-09-22"
toc: true
categories: [R, Probability, Statistics, Probabilistic graphical models, Mixture models, Gaussian mixture models]
image: "thumbnail.png"
---

A mixture of gaussians represents one of the most elegant and practically useful concepts in probability theory and machine learning. At its core, this model assumes that observed data arises from a combination of several underlying normal distributions, each contributing to the overall distribution with different weights and parameters. Rather than forcing all data points to conform to a single bell curve, a gaussian mixture allows for multiple clusters, each governed by its own mean, variance, and relative importance in the mixture.

The mathematical foundation rests on the idea that any data point could have originated from any of the component gaussians, but with varying probabilities. This creates a rich framework for modeling complex, multi-modal distributions that frequently appear in real-world scenarios. Consider, for example, the heights of adults in a population that includes both men and women, or the distribution of pixel intensities in an image containing multiple distinct objects. A single gaussian would poorly capture the true underlying structure, while a mixture model can naturally accommodate the multiple peaks and varying spreads that characterize such data. The beauty of this approach lies not only in its flexibility but also in its mathematical tractability, making it both theoretically sound and computationally feasible for a wide range of applications.

For example, suppose we were presented with the following data distribution, displayed here as a histogram:

```{python}
#| echo: false
#| include: false
#| warning: false
#| message: false

import scipy.stats as ss
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

np.random.seed(42)
plt.style.use("../../aux-files/custom-theme.mplstyle")
yellow = "#FFC300"
blue = "#3399FF"
pink = "#FF3399"
grey = "#121212"
white = "#E5E5E5"

```


```{python}
#| echo: false
#| code-fold: true 
#| fig-align: center

data = np.load("data.npy")

sns.histplot(data=data, color=yellow, alpha=1, ec=grey, zorder=2, stat="density")
plt.xlabel('x')
plt.ylabel('density')
plt.show()

```

Three clusters are immediately evident: the first with a peak around $x=10$, the second around $x=15$, and the third near $x=21$.

This synthetic dataset contains 1,000 observations sampled from a mixture of gaussians, also called a <span style="color: #FFC300;">*gaussian mixture model*</span> or <span style="color: #FFC300;">*GMM*</span>. In this introductory post, I'll explain the mathematical and graphical structure of the GMM that generated this data. Since I fixed the GMM parameters before sampling, we know the true underlying model. In a follow-up post, I'll tackle the inverse problem: given only the dataset, how can we estimate the parameters and recover the original GMM?